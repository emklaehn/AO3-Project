{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7407d031-f886-4d2d-8451-57a3b6648f81",
   "metadata": {},
   "source": [
    "# Webscraping Archive of Our Own (AO3) Data\n",
    "\n",
    "This code is from [Sopia Z.](https://medium.com/nerd-for-tech/mining-fanfics-on-ao3-part-1-data-collection-eac8b5d7a7fa) from Medium. Though, I did some modification to fit the needs for my project, specifically adding the fandoms, relationships, characters, and tags columns in the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### Importing Libraries\n",
    "\n",
    "Of the libraries given below, the only ones you may need to install in your environment is bs4 and pandas. Everything else should be in your standard Python library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e335a4-1e03-438e-90c6-2a9634176f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import urllib.request\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968e33f1-4aa1-4a4f-8439-ead11e811554",
   "metadata": {},
   "source": [
    "### Scraping the Data from AO3\n",
    "\n",
    "Before scraping the data, one needs to determine what type of data they want. If one decides to use the advance search/filter in AO3 for a specific subset of data, then the URL is going to be different fromt the standard AO3 URL.\n",
    "\n",
    "So, basically, once you know what you are needing, in my case, the last 4 weeks of fanfiction updates, then copy and paste the URL to find where \"page=#\", where # is the page number you are on in the web page. Then, in the getContent function below, split the URL like\n",
    "\n",
    "url = \"https://archiveofourown.org/some/random/gibberish/until&page=\" + str(i) + \"now/everything/after/the/page/number/if/there/is/more\"\n",
    "\n",
    "in the \"url\" variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaddf017-4f35-4c78-93e9-d95fa8ca3ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user-agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 11_2_2) AppleWebKit/605.1.15 (KHTML, like Gecko) Chromium/18.0.1025.168 Chrome/18.0.1025.168'}\n",
    "\n",
    "def getContent(url, start_page=1, end_page=1):\n",
    "    basic_url = url \n",
    "    #should be of the form: \"https://archiveofourown.org/tags/###TAG###/works?page=\"\n",
    "    \n",
    "    for i in range(start_page, end_page+1):\n",
    "        #url = basic_url+str(i)\n",
    "        url = \"https://archiveofourown.org/works/search?commit=Search&page=\"+str(i)+\"&work_search%5Bbookmarks_count%5D=&work_search%5Bcharacter_names%5D=&work_search%5Bcomments_count%5D=&work_search%5Bcomplete%5D=&work_search%5Bcreators%5D=&work_search%5Bcrossover%5D=&work_search%5Bfandom_names%5D=&work_search%5Bfreeform_names%5D=&work_search%5Bhits%5D=&work_search%5Bkudos_count%5D=&work_search%5Blanguage_id%5D=en&work_search%5Bquery%5D=&work_search%5Brating_ids%5D=&work_search%5Brelationship_names%5D=&work_search%5Brevised_at%5D=4+weeks+ago&work_search%5Bsingle_chapter%5D=0&work_search%5Bsort_column%5D=kudos_count&work_search%5Bsort_direction%5D=desc&work_search%5Btitle%5D=&work_search%5Bword_count%5D=\"\n",
    "        try:\n",
    "            req = urllib.request.Request(url,headers=headers)\n",
    "            resp = urllib.request.urlopen(req)\n",
    "            pageName = \"./Test/\"+str(i)+\".html\"\n",
    "            with open(pageName, 'w', encoding=\"utf-8\") as f:\n",
    "                f.write(resp.read().decode('utf-8'))        \n",
    "                print (pageName, end=\" \")\n",
    "            time.sleep(5)\n",
    "        except urllib.error.HTTPError as e:\n",
    "            if e.code == 429:\n",
    "                print('Too many requests!---SLEEPING---')\n",
    "                print('we should restart on page', i)\n",
    "                print('we should restart with this url:', url)\n",
    "                break\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c9e12e-5141-4e1b-bb5c-07712197cb48",
   "metadata": {},
   "source": [
    "### Using the Webscraping Function\n",
    "\n",
    "Include the starting number and ending page number given on the AO3 search results. There may be a case where it will stop before reaching the end number. If that happens, just change the command, where the start page is last page it got stuck on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a20d81d-14a5-489b-8140-e4d1f7ac4e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "getContent(\"e\",1967,2005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29476e05-b1e6-4344-a37f-4ca7e6ede48e",
   "metadata": {},
   "source": [
    "### Coverting the HTML Files into Useable Data\n",
    "\n",
    "This function will open the HTML files and scrape the data in the HTML files. After looking at all of the HTML files it will convert the data frame into a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffa0c95-2cdb-4442-8e97-b8d317dab46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_basic(page_content):\n",
    "    bs = BeautifulSoup(page_content, 'lxml')\n",
    "    titles = []\n",
    "    authors = []\n",
    "    ids = []\n",
    "    fandoms = []\n",
    "    date_updated = []\n",
    "    ratings = []\n",
    "    pairings = []\n",
    "    warnings = []\n",
    "    relationships = []\n",
    "    characters = []\n",
    "    tags = []\n",
    "    complete = []\n",
    "    languages = []\n",
    "    word_count = []\n",
    "    chapters = []\n",
    "    comments = []\n",
    "    kudos = []\n",
    "    bookmarks = []\n",
    "    hits = []\n",
    "\n",
    "    for article in bs.find_all('li', {'role':'article'}):\n",
    "        titles.append(article.find('h4', {'class':'heading'}).find('a').text)\n",
    "        try:\n",
    "            authors.append(article.find('a', {'rel':'author'}).text)\n",
    "        except:\n",
    "            authors.append('Anonymous')\n",
    "        ids.append(article.find('h4', {'class':'heading'}).find('a').get('href')[7:])\n",
    "        try:\n",
    "            result = []\n",
    "            for lists in article.find_all('h5', {'class':'fandoms heading'}):\n",
    "                for fandom in lists:\n",
    "                    if not fandom.find('a'):\n",
    "                        result.append(fandom.text)\n",
    "            result.pop(0)\n",
    "            fandoms.append(result)\n",
    "        except:\n",
    "            fandoms.append([])\n",
    "        date_updated.append(article.find('p', {'class':'datetime'}).text)\n",
    "        ratings.append(article.find('span', {'class':re.compile(r'rating\\-.*rating')}).text)\n",
    "        pairings.append(article.find('span', {'class':re.compile(r'category\\-.*category')}).text)\n",
    "        warnings.append(article.find('span', {'class':re.compile(r'warning\\-.*warnings')}).text)\n",
    "        try:\n",
    "            result = []\n",
    "            for lists in article.find_all('li', {'class':'relationships'}):\n",
    "                for relation in lists:\n",
    "                    result.append(relation.text)\n",
    "            relationships.append(result)\n",
    "        except:\n",
    "            relationships.append([])\n",
    "        try:\n",
    "            result = []\n",
    "            for lists in article.find_all('li', {'class':'characters'}):\n",
    "                for character in lists:\n",
    "                    result.append(character.text)\n",
    "            characters.append(result)\n",
    "        except:\n",
    "            characters.append([])\n",
    "        try:\n",
    "            result = []\n",
    "            for lists in article.find_all('li', {'class':'freeforms'}):\n",
    "                for tag in lists:\n",
    "                    result.append(tag.text)\n",
    "            tags.append(result)\n",
    "        except:\n",
    "            tags.append([])\n",
    "        complete.append(article.find('span', {'class':re.compile(r'complete\\-.*iswip')}).text)\n",
    "        languages.append(article.find('dd', {'class':'language'}).text)\n",
    "        count = article.find('dd', {'class':'words'}).text\n",
    "        if len(count) > 0:\n",
    "            word_count.append(count)\n",
    "        else:\n",
    "            word_count.append('0')\n",
    "        chapters.append(article.find('dd', {'class':'chapters'}).text.split('/')[0])\n",
    "        try:\n",
    "            comments.append(article.find('dd', {'class':'comments'}).text)\n",
    "        except:\n",
    "            comments.append('0')\n",
    "        try:\n",
    "            kudos.append(article.find('dd', {'class':'kudos'}).text)\n",
    "        except:\n",
    "            kudos.append('0')\n",
    "        try:\n",
    "            bookmarks.append(article.find('dd', {'class':'bookmarks'}).text)\n",
    "        except:\n",
    "            bookmarks.append('0')\n",
    "        try:\n",
    "            hits.append(article.find('dd', {'class':'hits'}).text)\n",
    "        except:\n",
    "            hits.append('0')\n",
    "\n",
    "    df = pd.DataFrame(list(zip(titles, authors, ids, fandoms, date_updated, ratings, pairings,\\\n",
    "                              warnings, relationships, characters, tags, complete, languages,\\\n",
    "                               word_count, chapters, comments, kudos, bookmarks, hits)))\n",
    "    \n",
    "    print('Successfully processed', len(df), 'rows!')\n",
    "    \n",
    "    with open('March2023_AO3.csv','a', encoding='utf8') as f:\n",
    "        df.to_csv(f, header=False, index=False)\n",
    "    temp = pd.read_csv('March2023_AO3.csv')\n",
    "    print('Now we have a total of', len(temp), 'rows of data!')\n",
    "    print('================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a547f783-5e91-4778-8c6a-db216eabd2d4",
   "metadata": {},
   "source": [
    "### Creating the CSV file\n",
    "\n",
    "This will give the CSV file headers and create the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eaa441-0284-4323-9119-a048c909e120",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['Title', 'Author', 'ID', 'Fandoms', 'Date_updated', 'Rating', 'Pairing', 'Warning', 'Relationships', 'Characters', 'Tags', 'Complete', 'Language', 'Word_count', 'Num_chapters', 'Num_comments', 'Num_kudos', 'Num_bookmarks', 'Num_hits']\n",
    "with open('March2023_AO3.csv','w', encoding='utf8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dfc066-c20f-43c0-b2c0-06a8f526afe2",
   "metadata": {},
   "source": [
    "### Using the Function\n",
    "\n",
    "This code will go page by page, given a maximum page number, and use the function to extract the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4d1a1d-66ed-4875-a8f9-f0217ca1b67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "totalPages = 2005\n",
    "for i in range(1, totalPages+1):\n",
    "    pageName = \"./Test/\"+str(i)+\".html\"\n",
    "    with open(pageName, mode='r', encoding='utf8') as f:\n",
    "        print('Now we are opening page', i, '...')\n",
    "        page = f.read()\n",
    "        process_basic(page)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
